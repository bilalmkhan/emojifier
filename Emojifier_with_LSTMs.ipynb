{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emojify \n",
    "\n",
    "The Emojifier automatically captures the sentiment of a sentence and displays it using an emoji. We will implement a model which inputs a sentence (such as \"Let's go see the baseball game tonight!\") and finds the most appropriate emoji to be used with this sentence (‚öæÔ∏è). Using word vectors, even if our training set explicitly relates only a few words to a particular emoji, the algorithm will be able to generalize and associate words in the test set to the same emoji even if those words don't even appear in the training set. This allows us to build an accurate classifier mapping from sentences to emojis, even using a small training set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset EMOJISET\n",
    "\n",
    "A very small dataset (X, Y) where:\n",
    "- X contains 127 sentences (strings)\n",
    "- Y contains a integer label between 0 and 4 corresponding to an emoji for each sentence\n",
    "\n",
    "We will load the dataset and it between training (127 examples) and testing (56 examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "\"\"\"\n",
    "Reads a csv data file\n",
    "\"\"\"\n",
    "def read_csv(filename = 'data/emojify_data.csv'):\n",
    "    phrase = []\n",
    "    emoji = []\n",
    "\n",
    "    with open (filename) as csvDataFile:\n",
    "        csvReader = csv.reader(csvDataFile)\n",
    "\n",
    "        for row in csvReader:\n",
    "            phrase.append(row[0])\n",
    "            emoji.append(row[1])\n",
    "\n",
    "    X = np.asarray(phrase)\n",
    "    Y = np.asarray(emoji, dtype=int)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, Y_train = read_csv('data/train_emoji.csv')\n",
    "X_test, Y_test = read_csv('data/tesss.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximum length of a sentence in the training set\n",
    "maxLen = len(max(X_train, key=len).split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the labels Y one-hot vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(Y, C):\n",
    "    \"\"\"\n",
    "    Converts Y to a matrix of one-hot vectors\n",
    "    \"\"\"\n",
    "    Y = np.eye(C)[Y.reshape(-1)]\n",
    "    return Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_oh_train = convert_to_one_hot(Y_train, C = 5)\n",
    "Y_oh_test = convert_to_one_hot(Y_test, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function `label_to_emoji(label)` that converts a label (int or string) into the corresponding emoji code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_dictionary = {\"0\": \"\\u2764\\uFE0F\",    # :heart: prints a black instead of red heart depending on the font\n",
    "                    \"1\": \":baseball:\",\n",
    "                    \"2\": \":smile:\",\n",
    "                    \"3\": \":disappointed:\",\n",
    "                    \"4\": \":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "def label_to_emoji(label):\n",
    "    \"\"\"\n",
    "    Converts a label (int or string) into the corresponding emoji code (string) ready to be printed\n",
    "    \"\"\"\n",
    "    return emoji.emojize(emoji_dictionary[str(label)], use_aliases=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how `label_to_emoji(label)` takes a label and converts it to emoji:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am proud of your achievements üòÑ\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(X_train[index], label_to_emoji(Y_train[index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. GloVe Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we read define a function to read the the GloVe vectors file. We will use 50-dimensional GloVe vectors to represent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_glove_vecs(glove_file):\n",
    "    with open(glove_file, 'r') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "        \n",
    "        i = 1\n",
    "        words_to_index = {}\n",
    "        index_to_words = {}\n",
    "        for w in sorted(words):\n",
    "            words_to_index[w] = i\n",
    "            index_to_words[i] = w\n",
    "            i = i + 1\n",
    "    return words_to_index, index_to_words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_index, index_to_word, word_to_vec_map = read_glove_vecs('data/glove.6B.50d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have loaded:\n",
    "- `word_to_index`: dictionary mapping from words to their indices in the vocabulary (400,001 words, with the valid indices ranging from 0 to 400,000)\n",
    "- `index_to_word`: dictionary mapping from indices to their corresponding words in the vocabulary\n",
    "- `word_to_vec_map`: dictionary mapping words to their GloVe vector representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model\n",
    "\n",
    "We will build an LSTM model that takes as input word sequences. This model will be able to take word ordering into account. Emojifier will use pre-trained word embeddings to represent words, but will feed them into an LSTM, whose job it is to predict the most appropriate emoji. \n",
    "\n",
    "First, we load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import pandas as pd\n",
    "import emoji\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Dropout, LSTM, Activation\n",
    "from keras.layers.embeddings import Embedding\n",
    "np.random.seed(1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 The Embedding layer\n",
    "\n",
    "In Keras, the embedding matrix is represented as a \"layer\", and maps positive integers (indices corresponding to words) into dense vectors of fixed size (the embedding vectors). It can be trained or initialized with a pretrained embedding. In this part, we will create  create an [Embedding()](https://keras.io/layers/embeddings/) layer in Keras, initialize it with the GloVe 50-dimensional vectors loaded earlier in the notebook. Because our training set is quite small, we will not update the word embeddings but will instead leave their values fixed. \n",
    "\n",
    "The `Embedding()` layer takes an integer matrix of size (batch size, max input length) as input. This corresponds to sentences converted into lists of indices (integers), as shown in the figure below. The largest integer (i.e. word index) in the input should be no larger than the vocabulary size. The layer outputs an array of shape (batch size, max input length, dimension of word vectors).\n",
    "\n",
    "The first step is to convert all your training sentences into lists of indices, and then zero-pad all these lists so that their length is the length of the longest sentence. \n",
    "\n",
    "We will implement the following function that converts X (array of sentences as strings) into an array of indices corresponding to words in the sentences. The output shape should be such that it can be given to `Embedding()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_to_indices(X, word_to_index, max_len):\n",
    "    \"\"\"\n",
    "    Converts an array of sentences (strings) into an array of indices corresponding to words in the sentences.\n",
    "    The output shape should be such that it can be given to `Embedding()`. \n",
    "    \n",
    "    Arguments:\n",
    "    X -- array of sentences (strings), of shape (m, 1)\n",
    "    word_to_index -- a dictionary containing the each word mapped to its index\n",
    "    max_len -- maximum number of words in a sentence. You can assume every sentence in X is no longer than this. \n",
    "    \n",
    "    Returns:\n",
    "    X_indices -- array of indices corresponding to words in the sentences from X, of shape (m, max_len)\n",
    "    \"\"\"\n",
    "    \n",
    "    m = X.shape[0]    # number of training examples\n",
    "    \n",
    "    # Initialize X_indices as a numpy matrix of zeros and the correct shape \n",
    "    X_indices = np.zeros((m,max_len))\n",
    "    \n",
    "    for i in range(m):    # loop over training examples\n",
    "        \n",
    "        # Convert the ith training sentence in lower case and split it into words\n",
    "        sentence_words = [word.lower() for word in X[i].split()]\n",
    "        \n",
    "        # Initialize j to 0\n",
    "        j = 0\n",
    "        \n",
    "        # Loop over the words of sentence_words\n",
    "        for w in sentence_words:\n",
    "            # Set the (i,j)th entry of X_indices to the index of the correct word\n",
    "            X_indices[i, j] = word_to_index[w]\n",
    "            # Increment j to j + 1\n",
    "            j = j + 1\n",
    "    \n",
    "    return X_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we implement `pretrained_embedding_layer()` that carries out the following steps:\n",
    "1. Initialize the embedding matrix as a numpy array of zeroes with the correct shape.\n",
    "2. Fill in the embedding matrix with all the word embeddings extracted from `word_to_vec_map`.\n",
    "3. Define Keras embedding layer. Use [Embedding()](https://keras.io/layers/embeddings/). Make this layer non-trainable, by setting `trainable = False` when calling `Embedding()`. If we were to set `trainable = True`, then it will allow the optimization algorithm to modify the values of the word embeddings. \n",
    "4. Set the embedding weights to be equal to the embedding matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrained_embedding_layer(word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Creates a Keras Embedding() layer and loads in pre-trained GloVe 50-dimensional vectors.\n",
    "    \n",
    "    Arguments:\n",
    "    word_to_vec_map -- dictionary mapping words to their GloVe vector representation.\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    embedding_layer -- pretrained layer Keras instance\n",
    "    \"\"\"\n",
    "    \n",
    "    vocab_len = len(word_to_index) + 1                  # adding 1 to fit Keras embedding\n",
    "    emb_dim = word_to_vec_map[\"cucumber\"].shape[0]      # define dimensionality of your GloVe word vectors\n",
    "    \n",
    "\n",
    "    # Initialize the embedding matrix as a numpy array of zeros of shape (vocab_len, dimensions of word vectors = emb_dim)\n",
    "    emb_matrix = np.zeros((vocab_len, emb_dim))\n",
    "    \n",
    "    # Set each row \"index\" of the embedding matrix to be the word vector representation of the \"index\"th word of the vocabulary\n",
    "    for word, index in word_to_index.items():\n",
    "        emb_matrix[index, :] = word_to_vec_map[word]\n",
    "\n",
    "    # Define Keras embedding layer\n",
    "    embedding_layer = Embedding(vocab_len, emb_dim, trainable = False)\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    # Build the embedding layer\n",
    "    embedding_layer.build((None,))\n",
    "    \n",
    "    # Set the weights of the embedding layer to the embedding matrix\n",
    "    embedding_layer.set_weights([emb_matrix])\n",
    "    \n",
    "    return embedding_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Building the Emojifier\n",
    "\n",
    "Next, we build the Emojifier model. We use the embedding layer we have built, and feed its output to an LSTM network. \n",
    "\n",
    "We will implement `Emojify`, which builds a Keras graph. The model takes as input an array of sentences of shape (`m`, `max_len`, ) defined by `input_shape`. It outputs a softmax probability vector of shape (`m`, `C = 5`). Wee use the following layers to build our network: `Input(shape = ..., dtype = '...')`, [LSTM()](https://keras.io/layers/recurrent/#lstm), [Dropout()](https://keras.io/layers/core/#dropout), [Dense()](https://keras.io/layers/core/#dense), and [Activation()](https://keras.io/activations/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Emojify(input_shape, word_to_vec_map, word_to_index):\n",
    "    \"\"\"\n",
    "    Function creating the Emojify model's graph.\n",
    "    \n",
    "    Arguments:\n",
    "    input_shape -- shape of the input, usually (max_len,)\n",
    "    word_to_vec_map -- dictionary mapping every word in a vocabulary into its 50-dimensional vector representation\n",
    "    word_to_index -- dictionary mapping from words to their indices in the vocabulary (400,001 words)\n",
    "\n",
    "    Returns:\n",
    "    model -- a model instance in Keras\n",
    "    \"\"\"\n",
    "\n",
    "    # Define sentence_indices shape input_shape and dtype 'int32' as the input of the graph\n",
    "    sentence_indices = Input(input_shape, dtype = 'int32')\n",
    "    \n",
    "    # Create the embedding layer pretrained with GloVe Vectors\n",
    "    embedding_layer = pretrained_embedding_layer(word_to_vec_map, word_to_index)\n",
    "    \n",
    "    # Propagate sentence_indices through your embedding layer, you get back the embeddings\n",
    "    embeddings = embedding_layer(sentence_indices)  \n",
    "    \n",
    "    # Propagate the embeddings through an LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output should be a batch of sequences\n",
    "    X = LSTM(128, return_sequences = True)(embeddings)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through another LSTM layer with 128-dimensional hidden state\n",
    "    # The returned output should be a single hidden state, not a batch of sequences.\n",
    "    X = LSTM(128, return_sequences=False)(X)\n",
    "    # Add dropout with a probability of 0.5\n",
    "    X = Dropout(0.5)(X)\n",
    "    # Propagate X through a Dense layer with softmax activation to get back a batch of 5-dimensional vectors.\n",
    "    X = Dense(5)(X)\n",
    "    # Add a softmax activation\n",
    "    X = Activation('softmax')(X)\n",
    "    \n",
    "    # Create Model instance which converts sentence_indices into X.\n",
    "    model = Model(inputs = sentence_indices, outputs = X)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our architecture uses \"20,223,927\" parameters, of which 20,000,050 (the word embeddings) are non-trainable, and the remaining 223,877 are. Because our vocabulary size has 400,001 words (with valid indices from 0 to 400,000) there are 400,001\\*50 = 20,000,050 non-trainable parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 10, 50)            20000050  \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 10, 128)           91648     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 10, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 128)               131584    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5)                 645       \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5)                 0         \n",
      "=================================================================\n",
      "Total params: 20,223,927\n",
      "Trainable params: 223,877\n",
      "Non-trainable params: 20,000,050\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Emojify((maxLen,), word_to_vec_map, word_to_index)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating the model in Keras, we compile it and define the loss, optimizer and metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we train the model. Emojifier `model` takes as input an array of shape (`m`, `max_len`) and outputs probability vectors of shape (`m`, `number of classes`). We thus have to convert X_train (array of sentences as strings) to X_train_indices (array of sentences as list of word indices), and Y_train (labels as indices) to Y_train_oh (labels as one-hot vectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_indices = sentences_to_indices(X_train, word_to_index, maxLen)\n",
    "Y_train_oh = convert_to_one_hot(Y_train, C = 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the Keras model on `X_train_indices` and `Y_train_oh`. We will use `epochs = 50` and `batch_size = 32`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "132/132 [==============================] - 2s 15ms/step - loss: 1.6083 - acc: 0.1970\n",
      "Epoch 2/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5326 - acc: 0.2955\n",
      "Epoch 3/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.5013 - acc: 0.3258\n",
      "Epoch 4/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.4393 - acc: 0.3561\n",
      "Epoch 5/100\n",
      "132/132 [==============================] - 0s 986us/step - loss: 1.3481 - acc: 0.4545\n",
      "Epoch 6/100\n",
      "132/132 [==============================] - 0s 976us/step - loss: 1.2347 - acc: 0.5152\n",
      "Epoch 7/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.1770 - acc: 0.4470\n",
      "Epoch 8/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 1.0555 - acc: 0.5682\n",
      "Epoch 9/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.8778 - acc: 0.7121\n",
      "Epoch 10/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.8236 - acc: 0.6970\n",
      "Epoch 11/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.7034 - acc: 0.7500\n",
      "Epoch 12/100\n",
      "132/132 [==============================] - 0s 915us/step - loss: 0.6005 - acc: 0.8030\n",
      "Epoch 13/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4934 - acc: 0.8333\n",
      "Epoch 14/100\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.5099 - acc: 0.8333\n",
      "Epoch 15/100\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.4794 - acc: 0.8258\n",
      "Epoch 16/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3543 - acc: 0.8636\n",
      "Epoch 17/100\n",
      "132/132 [==============================] - 0s 980us/step - loss: 0.3910 - acc: 0.8561\n",
      "Epoch 18/100\n",
      "132/132 [==============================] - 0s 952us/step - loss: 0.6492 - acc: 0.8182\n",
      "Epoch 19/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.5185 - acc: 0.8182\n",
      "Epoch 20/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3965 - acc: 0.8409\n",
      "Epoch 21/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.4700 - acc: 0.8182\n",
      "Epoch 22/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3883 - acc: 0.8636\n",
      "Epoch 23/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3774 - acc: 0.8561\n",
      "Epoch 24/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3049 - acc: 0.9091\n",
      "Epoch 25/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.3464 - acc: 0.8864\n",
      "Epoch 26/100\n",
      "132/132 [==============================] - 0s 954us/step - loss: 0.2423 - acc: 0.9394\n",
      "Epoch 27/100\n",
      "132/132 [==============================] - 0s 962us/step - loss: 0.3168 - acc: 0.8788\n",
      "Epoch 28/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2405 - acc: 0.9318\n",
      "Epoch 29/100\n",
      "132/132 [==============================] - 0s 2ms/step - loss: 0.3928 - acc: 0.8712\n",
      "Epoch 30/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.2669 - acc: 0.9091\n",
      "Epoch 31/100\n",
      "132/132 [==============================] - 0s 966us/step - loss: 0.2964 - acc: 0.8864\n",
      "Epoch 32/100\n",
      "132/132 [==============================] - 0s 963us/step - loss: 0.2036 - acc: 0.9394\n",
      "Epoch 33/100\n",
      "132/132 [==============================] - 0s 907us/step - loss: 0.2122 - acc: 0.9470\n",
      "Epoch 34/100\n",
      "132/132 [==============================] - 0s 898us/step - loss: 0.1577 - acc: 0.9621\n",
      "Epoch 35/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.1644 - acc: 0.9621\n",
      "Epoch 36/100\n",
      "132/132 [==============================] - 0s 981us/step - loss: 0.1890 - acc: 0.9394\n",
      "Epoch 37/100\n",
      "132/132 [==============================] - 0s 908us/step - loss: 0.1834 - acc: 0.9470\n",
      "Epoch 38/100\n",
      "132/132 [==============================] - 0s 901us/step - loss: 0.2186 - acc: 0.9318\n",
      "Epoch 39/100\n",
      "132/132 [==============================] - 0s 875us/step - loss: 0.1415 - acc: 0.9621\n",
      "Epoch 40/100\n",
      "132/132 [==============================] - 0s 907us/step - loss: 0.1578 - acc: 0.9545\n",
      "Epoch 41/100\n",
      "132/132 [==============================] - 0s 896us/step - loss: 0.0878 - acc: 0.9848\n",
      "Epoch 42/100\n",
      "132/132 [==============================] - 0s 887us/step - loss: 0.0822 - acc: 0.9773\n",
      "Epoch 43/100\n",
      "132/132 [==============================] - 0s 892us/step - loss: 0.0816 - acc: 0.9848\n",
      "Epoch 44/100\n",
      "132/132 [==============================] - 0s 866us/step - loss: 0.0501 - acc: 0.9924\n",
      "Epoch 45/100\n",
      "132/132 [==============================] - 0s 888us/step - loss: 0.0771 - acc: 0.9848\n",
      "Epoch 46/100\n",
      "132/132 [==============================] - 0s 891us/step - loss: 0.1002 - acc: 0.9773\n",
      "Epoch 47/100\n",
      "132/132 [==============================] - 0s 878us/step - loss: 0.1418 - acc: 0.9470\n",
      "Epoch 48/100\n",
      "132/132 [==============================] - 0s 886us/step - loss: 0.3068 - acc: 0.9242\n",
      "Epoch 49/100\n",
      "132/132 [==============================] - 0s 918us/step - loss: 0.1084 - acc: 0.9848\n",
      "Epoch 50/100\n",
      "132/132 [==============================] - 0s 890us/step - loss: 0.1719 - acc: 0.9545\n",
      "Epoch 51/100\n",
      "132/132 [==============================] - 0s 888us/step - loss: 0.1043 - acc: 0.9697\n",
      "Epoch 52/100\n",
      "132/132 [==============================] - 0s 889us/step - loss: 0.1231 - acc: 0.9470\n",
      "Epoch 53/100\n",
      "132/132 [==============================] - 0s 870us/step - loss: 0.0844 - acc: 0.9697\n",
      "Epoch 54/100\n",
      "132/132 [==============================] - 0s 872us/step - loss: 0.0638 - acc: 0.9773\n",
      "Epoch 55/100\n",
      "132/132 [==============================] - 0s 912us/step - loss: 0.0685 - acc: 0.9848\n",
      "Epoch 56/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0343 - acc: 1.0000\n",
      "Epoch 57/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0219 - acc: 1.0000\n",
      "Epoch 58/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0199 - acc: 1.0000\n",
      "Epoch 59/100\n",
      "132/132 [==============================] - 0s 934us/step - loss: 0.0301 - acc: 0.9924\n",
      "Epoch 60/100\n",
      "132/132 [==============================] - 0s 898us/step - loss: 0.0152 - acc: 1.0000\n",
      "Epoch 61/100\n",
      "132/132 [==============================] - 0s 898us/step - loss: 0.0157 - acc: 1.0000\n",
      "Epoch 62/100\n",
      "132/132 [==============================] - 0s 987us/step - loss: 0.0115 - acc: 1.0000\n",
      "Epoch 63/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0094 - acc: 1.0000\n",
      "Epoch 64/100\n",
      "132/132 [==============================] - 0s 950us/step - loss: 0.0126 - acc: 1.0000\n",
      "Epoch 65/100\n",
      "132/132 [==============================] - 0s 905us/step - loss: 0.0098 - acc: 1.0000\n",
      "Epoch 66/100\n",
      "132/132 [==============================] - 0s 900us/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 67/100\n",
      "132/132 [==============================] - 0s 917us/step - loss: 0.0056 - acc: 1.0000\n",
      "Epoch 68/100\n",
      "132/132 [==============================] - 0s 899us/step - loss: 0.0067 - acc: 1.0000\n",
      "Epoch 69/100\n",
      "132/132 [==============================] - 0s 909us/step - loss: 0.0058 - acc: 1.0000\n",
      "Epoch 70/100\n",
      "132/132 [==============================] - 0s 907us/step - loss: 0.0072 - acc: 1.0000\n",
      "Epoch 71/100\n",
      "132/132 [==============================] - 0s 905us/step - loss: 0.0048 - acc: 1.0000\n",
      "Epoch 72/100\n",
      "132/132 [==============================] - 0s 918us/step - loss: 0.0064 - acc: 1.0000\n",
      "Epoch 73/100\n",
      "132/132 [==============================] - 0s 909us/step - loss: 0.0035 - acc: 1.0000\n",
      "Epoch 74/100\n",
      "132/132 [==============================] - 0s 963us/step - loss: 0.0023 - acc: 1.0000\n",
      "Epoch 75/100\n",
      "132/132 [==============================] - 0s 916us/step - loss: 0.0040 - acc: 1.0000\n",
      "Epoch 76/100\n",
      "132/132 [==============================] - 0s 903us/step - loss: 0.0037 - acc: 1.0000\n",
      "Epoch 77/100\n",
      "132/132 [==============================] - 0s 898us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 78/100\n",
      "132/132 [==============================] - 0s 902us/step - loss: 0.0022 - acc: 1.0000\n",
      "Epoch 79/100\n",
      "132/132 [==============================] - 0s 912us/step - loss: 0.0024 - acc: 1.0000\n",
      "Epoch 80/100\n",
      "132/132 [==============================] - 0s 895us/step - loss: 0.0043 - acc: 1.0000\n",
      "Epoch 81/100\n",
      "132/132 [==============================] - 0s 926us/step - loss: 0.0027 - acc: 1.0000\n",
      "Epoch 82/100\n",
      "132/132 [==============================] - 0s 890us/step - loss: 0.0018 - acc: 1.0000\n",
      "Epoch 83/100\n",
      "132/132 [==============================] - 0s 915us/step - loss: 0.0036 - acc: 1.0000\n",
      "Epoch 84/100\n",
      "132/132 [==============================] - 0s 1ms/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 85/100\n",
      "132/132 [==============================] - 0s 917us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 86/100\n",
      "132/132 [==============================] - 0s 895us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 87/100\n",
      "132/132 [==============================] - 0s 885us/step - loss: 0.0021 - acc: 1.0000\n",
      "Epoch 88/100\n",
      "132/132 [==============================] - 0s 883us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 89/100\n",
      "132/132 [==============================] - 0s 944us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 90/100\n",
      "132/132 [==============================] - 0s 902us/step - loss: 0.0019 - acc: 1.0000\n",
      "Epoch 91/100\n",
      "132/132 [==============================] - 0s 894us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 92/100\n",
      "132/132 [==============================] - 0s 909us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 93/100\n",
      "132/132 [==============================] - 0s 890us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 94/100\n",
      "132/132 [==============================] - 0s 894us/step - loss: 0.0016 - acc: 1.0000\n",
      "Epoch 95/100\n",
      "132/132 [==============================] - 0s 893us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 96/100\n",
      "132/132 [==============================] - 0s 894us/step - loss: 0.0015 - acc: 1.0000\n",
      "Epoch 97/100\n",
      "132/132 [==============================] - 0s 917us/step - loss: 0.0017 - acc: 1.0000\n",
      "Epoch 98/100\n",
      "132/132 [==============================] - 0s 926us/step - loss: 0.0013 - acc: 1.0000\n",
      "Epoch 99/100\n",
      "132/132 [==============================] - 0s 894us/step - loss: 0.0012 - acc: 1.0000\n",
      "Epoch 100/100\n",
      "132/132 [==============================] - 0s 906us/step - loss: 0.0012 - acc: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x11922f4e0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_indices, Y_train_oh, epochs = 100, batch_size = 32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model performs close to **100% accuracy** on the training set. Next, we evaluate your model on the test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56/56 [==============================] - 0s 5ms/step\n",
      "\n",
      "Test accuracy =  0.8750000085149493\n"
     ]
    }
   ],
   "source": [
    "X_test_indices = sentences_to_indices(X_test, word_to_index, max_len = maxLen)\n",
    "Y_test_oh = convert_to_one_hot(Y_test, C = 5)\n",
    "loss, acc = model.evaluate(X_test_indices, Y_test_oh)\n",
    "print()\n",
    "print(\"Test accuracy = \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get an accuracy of around 87.5 percent. Let's look at some mislabelled examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected emoji:üòû prediction: work is hard\tüòÑ\n",
      "Expected emoji:üòû prediction: This girl is messing with me\t‚ù§Ô∏è\n",
      "Expected emoji:‚ù§Ô∏è prediction: I love taking breaks\tüòû\n",
      "Expected emoji:üòÑ prediction: you brighten my day\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: she is a bully\t‚ù§Ô∏è\n",
      "Expected emoji:üòû prediction: go away\tüç¥\n",
      "Expected emoji:üç¥ prediction: I did not have breakfast ‚ù§Ô∏è\n"
     ]
    }
   ],
   "source": [
    "C = 5\n",
    "y_test_oh = np.eye(C)[Y_test.reshape(-1)]\n",
    "X_test_indices = sentences_to_indices(X_test, word_to_index, maxLen)\n",
    "pred = model.predict(X_test_indices)\n",
    "for i in range(len(X_test)):\n",
    "    x = X_test_indices\n",
    "    num = np.argmax(pred[i])\n",
    "    if(num != Y_test[i]):\n",
    "        print('Expected emoji:'+ label_to_emoji(Y_test[i]) + ' prediction: '+ X_test[i] + label_to_emoji(num).strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we try it on some new examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = ['not feeling happy','have a good day','job well done','oh bummer',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not feeling happy üòû\n",
      "have a good day üòÑ\n",
      "job well done üòÑ\n",
      "oh bummer üòû\n"
     ]
    }
   ],
   "source": [
    "for phrase in x_test:\n",
    "    X_test_indices = sentences_to_indices(np.array([phrase]), word_to_index, maxLen)\n",
    "    print(phrase +' '+  label_to_emoji(np.argmax(model.predict(X_test_indices))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The current model still isn't very robust at understanding negation (like \"not happy\") because the training set is small and so doesn't have a lot of examples of negation. But if the training set were larger, the LSTM model would be much better at understanding such complex sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "nlp-sequence-models",
   "graded_item_id": "RNnEs",
   "launcher_item_id": "acNYU"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
